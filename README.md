# NMT GAN
Recently, various pre trained models show excellent performances in the NMT field. However, still not enough to replace human translation.
Fine Tuning based on a sufficiently large amount of data can be a solution, but it is not easy for everyone.
Furthermore, there is also Exposure Bias problem in Sequence Generation Model Training. Among the Sequence Generation Tasks, Neural Machine Translation's Exposure Bias is small, but it still acts as a hurdle to improve model performance. This repo covers a series of experiments to overcome the aforementioned limitations through two generative training strategies and compare them. Details for training strategies are covered below.

<br>
<br>


## Training Strategy

### Fine Tuning

<br>

### Generative Training

<br>

### SeqGAN Training
> SeqGAN is a concept that applies the concept of GAN to Sequence Generation Task. Just like GAN, generators and discriminators ddversally train to improve generator's generative ability.

<br>
<br>

## Experimental Setup

### Datasetup

<br>

### Model Setup

<br>

### Training Setup

<br>
<br>


Result

<br><br>

## How to run

<br><br>


## Reference

SeqGAN

<br>
