# TransSum

This repo compares the performance of dealing with long texts for each of the four Transformer-based models.
Each is Vanilla Transformer, Universal Transformer, Hierarchical Transformer, and Transformer XL.
Except for Vanilla Transformer, the latter three models are designed to better handle long text data.

Performance is compared based on the Abstractive Text Summarization Task.

<br>
<br>

## Architecture desc

### Vanilla Transformer

<br>

### Universal Transformer


<br>


### Hierarchical Transformer


<br>


### Transformer XL

<br>
<br>

## Training Setup

<br>
<br>


## How to run

<br>
<br>


## Reference

<br>
